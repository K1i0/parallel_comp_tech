# Параллельные вычислительные технологии
> Используемые функции MPI

| Функция | Аргументы | Описание |
| ------ | ------ | ------ |
| `MPI_Init()` | <ul>- | Инициализация параллельной части программы |
| `MPI_Finalize()` | <ul>- | Завершение параллельной части программы |
| `MPI_Commsize(MPI_Comm comm, int* size)` | <ul><li>comm - коммуникатор <li>~~OUT~~ size - размер группы (out-параметр)</li></ul> | Определение общего числа параллельных процессов в группе (коммуникаторе) _comm_ |
| `MPI_Comm_rank(MPI_Comm comm, int* rank)` | <ul><li>comm - коммуникатор <li>~~OUT~~ rank - номер процесса в группе</ul> | Определение номера процесса в группе _comm_ |
| `MPI_Isend(const void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request *request)` | <ul><li>buf - начальный адрес буфера <li>count - количество элементов в буфере <li>datatype - тип данных в буфере <li>dest - ранг получателя <li>tag - тэг сообщения, comm - коммуникатор, в котором выполняется обмен <li>~~OUT~~ request - запрос, необходимый для отслеживания состояния обмена</li></ul>  | Передача сообщения, аналогичная _MPI_Send_, однако возврат из подпрограммы происходит сразу после инициализации процесса передачи без ожидания обработки всего сообщения, находящегося в буфере _buf_ |
| `MPI_Reduce(void *sbuf, void *rbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm)` | <ul><li>адрес начала буфера аргументов<li>~~OUT~~ rbuf - адрес начала буфера-результата<li>count - количество аргументов для передачи у каждого процесса<li>datatype - тип передаваемых элементов<li>op - идентификатор глобальной операции, выполняемой над аргументами<li>root - процесс-получатель результата<li>comm - идентификатор группы</li></ul>  | Объединение элементов входного буфера каждого процесса в группе, используя операцию _op_ , и возвращает объединенное значение в выходной буфер процесса с номером _root_ |
| `MPI_Irecv(void *buf, int count, MPI_Datatype datatype, int source, int msgtag, MPI_Comm comm, MPI_Request *request)` | <ul><li>~~OUT~~ buf - адрес начала буфера приема сообщения<li>count - максимальное число элементов в принимаемом сообщении<li>datatype - тип элементов принимаемого сообщения<li>source - номер процесса-отправителя, msgtag - идентификатор принимаемого сообщения<li>comm - коммуникатор группы<li>~~OUT~~ request - идентификатор асинхронного приема сообщения</li></ul> | Прием сообщения, аналогичный _MPI_Recv_, однако возврат из подпрограммы происходит сразу после инициализации процесса приема без ожидания получения сообщения в буфере _buf_. Окончание процесса приема можно определить с помощью параметра request и процедур _MPI_Wait_ и _MPI_Test_. |
| `MPI_Waitall(int count, MPI_Request *requests, MPI_Status *statuses)` | <ul><li>count - число идентификаторов ожидаемых процессов<li>requests - массив дескрипторов асинхронного приема или передачи<li>~~OUT~~ statuses - статусы сообщений</li></ul> | Выполнение процесса блокируется до тех пор, пока все операции обмена, ассоциированные с указанными идентификаторами, не будут завершены |

> [Документация MPI](https://curc.readthedocs.io/en/latest/programming/MPI-C.html)
> Больше информации об интерфейсе передачи сообщений.

## Оформление результатов выполнения работы
> Для оформления результатов работы (построения графиков) используется библиотека [Matplotlib](https://matplotlib.org/stable/)

### Пример построенного графика
![Пример графика](/doc/exchange_time/result.png)
### Установка
```sh
pip install matplotlib
```

### Компиляция и запуск
```sh
mpicc -o main ./src/exchange_time/extime.c
```

> Более подробное описание параметров MPI можно посмотреть [туть](https://www.open-mpi.org/doc/v4.1/man1/mpiexec.1.php) или [вот туть](https://docs.open-mpi.org/en/v5.0.x/man-openmpi/man1/mpirun.1.html#label-schizo-ompi-map-by)

| Параметр | Описание |
| ------ | ------ |
| --report-bindings | Визуализация привязок процессов (к узлам, сокетам, ядрам) |
| -np | Регулировка количества запущенных процессов |
| sinfo | Просмотр информации о кластере |
| -host | Выбор узлов, на которых запускается программа |
| --map-by | Выбор способа привязкип процессов (к ядрам, сокетам) |
| squeue | Просмотр очереди задач на кластере |

>  Запуск на кластере (на уровне оперативной памяти, межпроцессорной шины, сетевом), где size - размер передаваемого сообщения в МБ


```sh
mpiexec --report-bindings -np 2 -host oak-cn3.ipa.csc.sibsutis.ru:2 --map-by core ./main <size>
mpiexec --report-bindings -np 2 -host oak-cn3.ipa.csc.sibsutis.ru:2 --map-by socket ./main <size>
mpiexec --report-bindings -np 2 -host oak.cnl.ipa.csc.sibsutis.ru,oak-cn3.ipa.csc.sibsutis.ru ./main <size>
```